{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size, embbeding_size, filter_size, num_filters):\n",
    "#         sequence_length – The length of our sentences. Remember that we padded all \\\n",
    "#       our sentences to have the same length (59 for our data set).\n",
    "#         num_classes – Number of classes in the output layer, two in our case (positive and negative).\n",
    "#         vocab_size – The size of our vocabulary. This is needed to define the size of our embedding layer, \\\n",
    "#       which will have shape [vocabulary_size, embedding_size].\n",
    "#         embedding_size – The dimensionality of our embeddings.\n",
    "#         filter_sizes – The number of words we want our convolutional filters to cover. \\\n",
    "#       We will have num_filters for each size specified here. For example, [3, 4, 5] \\\n",
    "#       means that we will have filters that slide over 3, 4 and 5 words respectively, for a total of 3 * num_filters filters.\n",
    "#       num_filters – The number of filters per filter size (see above).\n",
    "\n",
    "\n",
    "        #Placeholders\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n",
    "        \n",
    "        \n",
    "#         Embedding layer\n",
    "#         The first layer we define is the embedding layer, which maps vocabulary word indices into \\\n",
    "#         low-dimensional vector representations. It’s essentially a lookup table that we learn from data.\n",
    "\n",
    "        with tf.device('/cpu:0', tf.name_scope(\"embedding\")):\n",
    "            W = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size,embbeding_size], -1.0, 1.0), \n",
    "                    name=\"W\")\n",
    "            # tf.nn.embedding_lookup creates the actual embedding operation\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W,input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "    \n",
    "                    \n",
    "#           Convolution layer\n",
    "#           TensorFlow’s convolutional conv2d operation expects a 4-dimensional \\\n",
    "#           tensor with dimensions corresponding to batch, width, height and channel.\\\n",
    "#           The result of our embedding doesn’t contain the channel dimension, so we add it manually, \\\n",
    "#           leaving us with a layer of shape [None, sequence_length, embedding_size, 1]. ---> \\\n",
    "#           Em imagens, por exemplo, seria 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#           Now we’re ready to build our convolutional layers followed by max-pooling. \\\n",
    "#           Remember that we use filters of different sizes. Because each convolution \\\n",
    "#           produces tensors of different shapes we need to iterate through them, \\\n",
    "#           create a layer for each of them, and then merge the results into one big feature vector\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-max-poll-{}\".format(filter_size)):\n",
    "                #Convolution layer\n",
    "                #filtro especifico, tamanho do embbeding, numero de canais e todos os filtros\n",
    "                filter_shape = [filter_size, embbeding_size, 1, num_filters] \n",
    "                #Each filter slides over all embbeding matrix, but varies how many words each one will slide\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape,stdev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                #\"VALID\" padding means that we slide the filter over our sentence without padding the \\\n",
    "                #edges, performing a narrow convolution that gives us an output \\\n",
    "                #of shape [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\"\n",
    "                )\n",
    "                #Activation function - apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.add_bias(conv,b), name=\"relu\")\n",
    "                #Max pooling\n",
    "                pooled = tf.nn.max_pool(\n",
    "                            h,\n",
    "                            k_size=[1, sequence_length - filter_size + 1,1,1],\n",
    "                            strides = [1,1,1,1],\n",
    "                            padding = \"VALID\",\n",
    "                            name= \"pool\")\n",
    "                #Performing max-pooling over the output of a specific filter size \\\n",
    "                # leaves us with a tensor of shape [batch_size, 1, 1, num_filters]\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "\n",
    "                \n",
    "        #      Combine all pooled filters\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3,pooled_outputs)\n",
    "        #une a porra toda num vetor so\n",
    "        #Once we have all the pooled output tensors from each filter size we \\\n",
    "        #combine them into one long feature vector of shape [batch_size, num_filters_total]\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "        #outputs\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([num_filters_total,num_classes]), stddev=0.1, name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_prob, W, b, name=\"scores\")\n",
    "            #argamx = retorna o indice com o maior valor\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        #Calculate mean-cross-entropy loss\n",
    "        #, tf.nn.softmax_cross_entropy_with_logits is a convenience function that calculates the \\\n",
    "        # cross-entropy loss for each class, given our scores and the correct input labels\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores, self.input_y)\n",
    "            #take mean\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "            \n",
    "            \n",
    "        #Take Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        # allow_soft_placement setting allows TensorFlow to fall back on a device with a certain operation \\\n",
    "        #implemented when the preferred device doesn’t exist\n",
    "        \n",
    "        # log_device_placement is set, TensorFlow log on which devices (CPU or GPU) it places operations. \n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement\n",
    "    )\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=2,\n",
    "            vocab_size=len(vocabulary),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=map(int, FLAGS.filter_sizes.split(\",\")),\n",
    "            num_filters=FLAGS.num_filters\n",
    "        )\n",
    "        \n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optmizer = tf.train.AdamOptmizer(1e-4)\n",
    "        grads_and_vars = optmizer.compute_gradients(cnn.loss)\n",
    "        #train_op here is a newly created operation that we can run to perform a gradient update on our parameters.\n",
    "        train_op = optmizer.apply_gradients(grads_and_vars,global_step=global_step)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
